#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
解析興化語聖經文本（第四版）
支援新的格式化文本：每節一行，有標題和段落標題
"""

import re
import json
import sys
import shutil
from pathlib import Path
from book_info import ALL_BOOKS, HAN_TO_ROM, ROM_TO_HAN, HAN_TO_ENG

def number_to_chinese(n):
    """阿拉伯數字轉漢字"""
    if n == 0:
        return '零'

    digits = ['', '一', '二', '三', '四', '五', '六', '七', '八', '九']

    if n < 10:
        return digits[n]
    elif n < 20:
        return '十' + (digits[n-10] if n > 10 else '')
    elif n < 100:
        tens = digits[n // 10]
        ones = digits[n % 10]
        return tens + '十' + (ones if ones else '')
    elif n < 1000:
        hundreds = digits[n // 100]
        remainder = n % 100
        result = hundreds + '百'
        if remainder > 0:
            if remainder < 10:
                result += '零' + digits[remainder]
            else:
                result += number_to_chinese(remainder)
        return result
    else:
        return str(n)  # 超過1000就用阿拉伯數字


def tokenize_rom(text):
    """
    將羅馬字文本分割為詞 tokens
    返回：[{'type': 'word', 'text': 'Kî-táu'}, {'type': 'punct', 'text': ','}, ...]
    """
    tokens = []
    i = 0
    current_word = ""

    while i < len(text):
        char = text[i]

        # 標點符號（包含 ASCII 和 Unicode 引號）
        if char in '.,;:!?\'"()[]""''…—""''\u201C\u201D\u2018\u2019':
            # 保存當前詞
            if current_word.strip():
                tokens.append({
                    'type': 'word',
                    'text': current_word.strip()
                })
                current_word = ""

            # 跳過標點（不添加到 tokens，因為漢字版標點不同）
            i += 1
            continue

        # 空格：詞的分界
        if char == ' ':
            if current_word.strip():
                tokens.append({
                    'type': 'word',
                    'text': current_word.strip()
                })
                current_word = ""
            i += 1
            continue

        # 一般字符
        current_word += char
        i += 1

    # 保存最後的詞
    if current_word.strip():
        tokens.append({
            'type': 'word',
            'text': current_word.strip()
        })

    return tokens


def tokenize_han(text):
    """
    將漢字文本分割為 tokens
    返回：[{'type': 'char', 'text': '起'}, {'type': 'compound', 'text': '第一', 'chars': ['第', '一']}, ...]
    """
    tokens = []
    i = 0

    while i < len(text):
        char = text[i]

        # 專名標記（跳過）
        if char == '{':
            end = text.find('}', i)
            if end != -1:
                # 找到專名，記錄專名內容
                proper_name = text[i+1:end]
                # 拆分專名中的每個字
                for c in proper_name:
                    if c in '。，、；：！？（）「」…':
                        tokens.append({'type': 'punct', 'text': c})
                    elif c.strip():
                        tokens.append({'type': 'char', 'text': c, 'proper_name': True})
                i = end + 1
                continue

        # 合音字開始（「」標記）
        if char == '「':
            end = text.find('」', i)
            if end != -1:
                compound = text[i+1:end]
                tokens.append({
                    'type': 'compound',
                    'text': compound,
                    'chars': list(compound)
                })
                i = end + 1
                continue

        # 標點符號（包含中文和 Unicode 引號）
        if char in '。，、；：！？（）「」『』【】…""''\u201C\u201D\u2018\u2019':
            tokens.append({
                'type': 'punct',
                'text': char
            })
            i += 1
            continue

        # 空白
        if char.strip() == '':
            i += 1
            continue

        # 一般漢字
        tokens.append({
            'type': 'char',
            'text': char
        })
        i += 1

    return tokens


def extract_proper_names(han_text):
    """
    提取專名標記 {...}
    返回：(乾淨文本, {位置: 長度})
    """
    proper_names = {}  # {start_pos: length}
    clean_text = ""
    offset = 0

    i = 0
    while i < len(han_text):
        if han_text[i] == '{':
            # 找到專名開始
            j = i + 1
            while j < len(han_text) and han_text[j] != '}':
                j += 1

            if j < len(han_text):
                # 找到配對的 }
                proper_name = han_text[i+1:j]
                proper_names[len(clean_text)] = len(proper_name)
                clean_text += proper_name
                i = j + 1
            else:
                # 沒有配對，保留 {
                clean_text += han_text[i]
                i += 1
        else:
            clean_text += han_text[i]
            i += 1

    return clean_text, proper_names


def extract_compound_chars(han_text):
    """
    提取合音字標記 「...」
    返回：(乾淨文本, {位置: 長度})
    """
    compounds = {}  # {start_pos: length}
    clean_text = ""

    i = 0
    while i < len(han_text):
        if han_text[i] == '「':
            # 找到合音字開始
            j = i + 1
            while j < len(han_text) and han_text[j] != '」':
                j += 1

            if j < len(han_text):
                # 找到配對的 」
                compound = han_text[i+1:j]
                compounds[len(clean_text)] = len(compound)
                clean_text += compound
                i = j + 1
            else:
                # 沒有配對，保留 「
                clean_text += han_text[i]
                i += 1
        else:
            clean_text += han_text[i]
            i += 1

    return clean_text, compounds


def align_tokens(han_text, rom_text):
    """
    對齊漢字和羅馬字，生成 token 陣列

    規則：
    - 1 個漢字 = 1 個羅馬字音節
    - 1 個合音字（多個漢字在「」中）= 1 個羅馬字音節
    - 多音節羅馬字詞（用 - 連接）= 對應數量的漢字
    - 標點以漢字為主

    返回：[{"type": "word", "han": "...", "rom": "...", "form": "..."}, ...]
    """
    # Tokenize
    han_tokens = tokenize_han(han_text)
    rom_tokens = tokenize_rom(rom_text)

    # 提取羅馬字詞（忽略標點）
    rom_words = [t for t in rom_tokens if t['type'] == 'word']

    aligned = []
    h_idx = 0  # han index
    r_idx = 0  # rom index

    while h_idx < len(han_tokens):
        h_token = han_tokens[h_idx]

        # 標點符號：直接添加（只用漢字版本的標點）
        if h_token['type'] == 'punct':
            aligned.append({
                'type': 'punct',
                'han': h_token['text'],
                'rom': ''
            })
            h_idx += 1
            continue

        # 一般字或合音字
        if r_idx < len(rom_words):
            rom_word = rom_words[r_idx]['text']

            # 計算羅馬字音節數量
            rom_syllable_count = len(rom_word.split('-'))

            # 檢查是否是專名
            is_proper_name = h_token.get('proper_name', False)

            if h_token['type'] == 'compound':
                # 合音字：多個漢字對應1個羅馬字音節
                form = 'compound_single'
                han_text = h_token['text']
                h_idx += 1
            elif h_token['type'] == 'char':
                # 一般字
                if rom_syllable_count == 1:
                    form = 'single'  # 1音節1字
                    han_text = h_token['text']
                    h_idx += 1
                else:
                    # 多音節詞：需要收集多個漢字
                    # 檢查是否所有字符都有連字號
                    if '-' in rom_word:
                        form = 'phrase'
                    else:
                        form = 'compound'

                    chars = [h_token['text']]
                    h_idx += 1

                    # 收集剩餘的漢字（音節數-1個）
                    # 跳過中間的標點
                    collected = 1
                    while collected < rom_syllable_count and h_idx < len(han_tokens):
                        if han_tokens[h_idx]['type'] == 'char':
                            chars.append(han_tokens[h_idx]['text'])
                            collected += 1
                            h_idx += 1
                        elif han_tokens[h_idx]['type'] == 'compound':
                            # 遇到合音字，也算一個音節
                            chars.append(han_tokens[h_idx]['text'])
                            collected += 1
                            h_idx += 1
                        elif han_tokens[h_idx]['type'] == 'punct':
                            # 跳過標點，不計入詞中
                            h_idx += 1
                        else:
                            break

                    han_text = ''.join(chars)

            token = {
                'type': 'word',
                'han': han_text,
                'rom': rom_word,
                'form': form
            }

            # 添加專名標記
            if is_proper_name:
                token['proper_name'] = True

            aligned.append(token)
            r_idx += 1
        else:
            # 羅馬字已用完，漢字多出來的
            token = {
                'type': 'word',
                'han': h_token['text'],
                'rom': '',
                'form': 'single' if h_token['type'] == 'char' else 'compound_single'
            }
            if h_token.get('proper_name'):
                token['proper_name'] = True

            aligned.append(token)
            h_idx += 1

    # 如果羅馬字還有剩餘
    while r_idx < len(rom_words):
        rom_word = rom_words[r_idx]['text']
        rom_syllable_count = len(rom_word.split('-'))

        aligned.append({
            'type': 'word',
            'han': '',
            'rom': rom_word,
            'form': 'single' if rom_syllable_count == 1 else 'phrase'
        })
        r_idx += 1

    return aligned


def parse_structured_text(file_path):
    """
    解析格式化的文本文件
    返回：{book_name: {chapter_num: {section_titles: [...], verses: {verse_num: text}}}}
    """
    with open(file_path, 'r', encoding='utf-8') as f:
        lines = f.readlines()

    result = {}
    current_book = None
    current_chapter = None
    current_verse_buffer = []
    pending_section_titles = []  # 暫存在章節之前出現的段落小標

    def flush_verse():
        """完成當前節的收集"""
        if current_verse_buffer and current_book and current_chapter is not None:
            verse_num = current_verse_buffer[0]
            verse_text = ''.join(current_verse_buffer[1:])

            if current_chapter not in result[current_book]['chapters']:
                result[current_book]['chapters'][current_chapter] = {
                    'section_titles': [],
                    'verses': {}
                }

            result[current_book]['chapters'][current_chapter]['verses'][verse_num] = verse_text
            current_verse_buffer.clear()

    for line in lines:
        line = line.rstrip('\n')

        # 空行
        if not line.strip():
            flush_verse()
            continue

        # 一級標題：書名
        if line.startswith('# '):
            flush_verse()
            book_name = line[2:].strip()
            current_book = book_name
            result[current_book] = {'chapters': {}}
            current_chapter = None
            pending_section_titles = []
            continue

        # 三級標題：段落小標
        if line.startswith('### '):
            flush_verse()
            section_title = line[4:].strip()
            if current_book and current_chapter is not None:
                # 章節已經存在，直接添加
                if current_chapter not in result[current_book]['chapters']:
                    result[current_book]['chapters'][current_chapter] = {
                        'section_titles': [],
                        'verses': {}
                    }
                result[current_book]['chapters'][current_chapter]['section_titles'].append(section_title)
            elif current_book:
                # 章節還未出現，暫存
                pending_section_titles.append(section_title)
            continue

        # 二級標題：章號
        if line.startswith('## '):
            flush_verse()
            chapter_str = line[3:].strip()
            try:
                current_chapter = int(chapter_str)
                if current_book:
                    if current_chapter not in result[current_book]['chapters']:
                        result[current_book]['chapters'][current_chapter] = {
                            'section_titles': [],
                            'verses': {}
                        }
                    # 將暫存的段落小標添加到這個章節
                    if pending_section_titles:
                        result[current_book]['chapters'][current_chapter]['section_titles'].extend(pending_section_titles)
                        pending_section_titles = []
            except ValueError:
                pass
            continue

        # 經節行
        match = re.match(r'^(\d+)\s+(.*)', line)
        if match:
            flush_verse()
            verse_num = int(match.group(1))
            verse_content = match.group(2)
            current_verse_buffer = [verse_num, verse_content]
        else:
            # 續行（詩歌體）
            if current_verse_buffer:
                current_verse_buffer.append('\n' + line)

    # 處理最後一節
    flush_verse()

    return result





def merge_and_generate_json(han_data, rom_data, output_file):
    """
    合併漢字和羅馬字資料，生成 bible_data.json
    按照聖經書目順序（從 book_info.py 的 ALL_BOOKS）輸出
    """
    books = []
    used_rom_books = set()

    for rom_name, han_name, eng_name, page in ALL_BOOKS:
        book = {
            "name_han": han_name,
            "name_rom": rom_name,
            "name_eng": eng_name,
            "chapters": []
        }

        # Case 1: 英文序
        if eng_name == 'Foreword':
            print(f"處理序言: {eng_name}")
            with open('data/foreword-en.txt', 'r', encoding='utf-8') as f:
                content = f.read()
            
            # 以一個或多個空行分割段落
            paragraphs = [p.strip().replace('\n', ' ') for p in re.split(r'\n\s*\n', content) if p.strip()]
            
            sections = []
            for i, para_text in enumerate(paragraphs):
                section = {
                    "type": "verse",
                    "verse": i + 1,  # 用段落序號作為 verse number
                    "rom": para_text,
                    "han": "",
                    "tokens": [] # 英文序沒有 tokens
                }
                sections.append(section)

            chapter = {
                "chapter": 1,
                "chapter_title_han": "",
                "chapter_title_rom": "",
                "sections": sections
            }
            book['chapters'].append(chapter)
            books.append(book)
            continue

        # Case 2: 興化語序
        if eng_name == 'Preface':
            print(f"處理序言: {eng_name}")
            with open('data/foreword-cpx.txt', 'r', encoding='utf-8') as f:
                content = f.read()

            rom_part = ""
            han_part = ""
            
            # 分割羅馬字和漢字部分
            if "# 序" in content:
                parts = content.split("# 序")
                rom_part = parts[0].replace("# Sṳ̄.", "").strip()
                han_part = parts[1].strip() if len(parts) > 1 else ""

            # 以一個或多個空行分割段落
            rom_paras = [p.strip() for p in re.split(r'\n\s*\n', rom_part) if p.strip()]
            han_paras = [p.strip() for p in re.split(r'\n\s*\n', han_part) if p.strip()]

            sections = []
            # 遍歷段落，假設羅馬字和漢字段落數目一致
            num_paras = max(len(rom_paras), len(han_paras))
            for i in range(num_paras):
                rom_text = rom_paras[i] if i < len(rom_paras) else ""
                han_text = han_paras[i] if i < len(han_paras) else ""

                # 移除專名和合音字標記，得到乾淨的顯示文本
                han_clean, _ = extract_proper_names(han_text)
                han_clean, _ = extract_compound_chars(han_clean)

                section = {
                    "type": "verse",
                    "verse": i + 1,
                    "rom": rom_text.replace('\n', ' '),
                    "han": han_clean.replace('\n', ' '),
                    "tokens": align_tokens(han_text, rom_text) if rom_text and han_text else []
                }
                sections.append(section)

            chapter = {
                "chapter": 1,
                "chapter_title_han": "",
                "chapter_title_rom": "",
                "sections": sections
            }
            book['chapters'].append(chapter)
            books.append(book)
            continue

        # Case 3: 一般聖經書卷
        if han_name in han_data:
            used_rom_books.add(rom_name)
            han_book_data = han_data[han_name]
            rom_book_data = rom_data.get(rom_name, {'chapters': {}})

            # 遍歷章節
            for chapter_num in sorted(han_book_data['chapters'].keys()):
                han_chapter = han_book_data['chapters'][chapter_num]
                rom_chapter = rom_book_data.get('chapters', {}).get(chapter_num, {'section_titles': [], 'verses': {}})

                chapter = {
                    "chapter": chapter_num,
                    "chapter_title_han": f"第{number_to_chinese(chapter_num)}章",
                    "chapter_title_rom": f"Dā̤ {chapter_num} Ca̤uⁿ",
                    "sections": []
                }

                # 添加段落標題
                for section_title_han in han_chapter.get('section_titles', []):
                    section = {
                        "type": "section_title",
                        "han": section_title_han,
                        "rom": "",  # 目前羅馬字版沒有段落標題
                        "tokens": align_tokens(section_title_han, "")
                    }
                    chapter['sections'].append(section)

                # 添加經節
                for verse_num in sorted(han_chapter['verses'].keys()):
                    han_text = han_chapter['verses'][verse_num]
                    rom_text = rom_chapter.get('verses', {}).get(verse_num, "")

                    han_clean, _ = extract_proper_names(han_text)
                    han_clean, _ = extract_compound_chars(han_clean)

                    verse = {
                        "type": "verse",
                        "verse": verse_num,
                        "rom": rom_text,
                        "han": han_clean,
                        "tokens": align_tokens(han_text, rom_text) if rom_text else []
                    }
                    chapter['sections'].append(verse)
                
                book['chapters'].append(chapter)
            books.append(book)

    # 生成最終 JSON
    result = {"books": books}

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)

    # 自動複製到網站目錄
    website_output = Path('website/public/bible_data.json')
    if website_output.parent.exists():
        shutil.copy2(output_file, website_output)
        print(f"  已複製至: {website_output}")
    else:
        print(f"  警告: website/public/ 目錄不存在，跳過複製")

    # 統計
    total_verses_han = sum(
        len(ch['verses'])
        for bk_name, bk in han_data.items()
        for ch_num, ch in bk['chapters'].items() if bk_name in HAN_TO_ROM
    )
    total_verses_rom = sum(
        len(ch['verses'])
        for bk_name, bk in rom_data.items()
        for ch_num, ch in bk['chapters'].items() if bk_name in ROM_TO_HAN
    )
    total_chapters_han = sum(len(bk['chapters']) for bk_name, bk in han_data.items() if bk_name in HAN_TO_ROM)
    total_chapters_rom = sum(len(bk['chapters']) for bk_name, bk in rom_data.items() if bk_name in ROM_TO_HAN)

    print(f"解析完成")
    print(f"  書卷數: {len(books)}")
    print(f"    - 漢字書卷: {len(han_data)}")
    print(f"    - 羅馬字書卷: {len(rom_data)}")
    print(f"    - 配對成功: {len(used_rom_books)}")
    print(f"  章節數: {total_chapters_han} (漢) / {total_chapters_rom} (羅)")
    print(f"  經節數: {total_verses_han} (漢) / {total_verses_rom} (羅)")
    print(f"  輸出: {output_file}")


def main():
    han_file = 'data/han.txt'
    rom_file = 'data/rom.txt'
    output_file = 'data/bible_data.json'

    if len(sys.argv) > 1:
        han_file = sys.argv[1]
    if len(sys.argv) > 2:
        rom_file = sys.argv[2]
    if len(sys.argv) > 3:
        output_file = sys.argv[3]

    print("解析漢字版 (han.txt)...")
    han_data = parse_structured_text(han_file)

    print("解析羅馬字版 (rom.txt)...")
    rom_data = parse_structured_text(rom_file)

    print("合併並生成 JSON...")
    merge_and_generate_json(han_data, rom_data, output_file)


if __name__ == '__main__':
    main()
